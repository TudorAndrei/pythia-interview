{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pretty_errors\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers.models.auto.tokenization_auto import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import TYPES, TYPES_DICT\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import Tensor\n",
    "from torch.nn import Dropout, Linear, Module\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.nn.modules.sparse import Embedding\n",
    "from torch.nn.modules.transformer import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from transformers.models.auto.modeling_auto import AutoModelForSequenceClassification\n",
    "\n",
    "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.functional.classification.f_beta import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BaseDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        X_y,\n",
    "        tokenizer_path: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.X, self.y = X_y\n",
    "        self.tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path)\n",
    "\n",
    "    def get_label_array(self, idx):\n",
    "        array = []\n",
    "        for i, col in enumerate(TYPES):\n",
    "            bin = self.y.iloc[idx, i]\n",
    "            array.append(TYPES_DICT[col][bin])\n",
    "\n",
    "        return array\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.X[idx]\n",
    "        label = self.get_label_array(idx)\n",
    "        text_encoded = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return {\n",
    "            \"ids\": text_encoded[\"input_ids\"].squeeze(),\n",
    "            \"mask\": text_encoded[\"attention_mask\"],\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float),\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tokenizer_path: str = None,\n",
    "        num_workers: int = 1,\n",
    "        batch_size: int = 4,\n",
    "        model_name: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_workers = num_workers\n",
    "        self.batch_size = batch_size\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer_path = tokenizer_path\n",
    "        self.data = pd.read_csv(data_path)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            self.data[\"posts\"], self.data[TYPES], stratify=self.data[TYPES]\n",
    "        )\n",
    "        self.train_dataset = BaseDataset(\n",
    "            (self.X_train.values, self.y_train.reset_index(drop=True)),\n",
    "            self.tokenizer_path,\n",
    "        )\n",
    "        self.test_dataset = BaseDataset(\n",
    "            (self.X_test.values, self.y_test.reset_index(drop=True)),\n",
    "            self.tokenizer_path,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file models/tokenizer/config.json not found\n",
      "file models/tokenizer/config.json not found\n",
      "file models/tokenizer/config.json not found\n",
      "file models/tokenizer/config.json not found\n"
     ]
    }
   ],
   "source": [
    "dm = DataModule(data_path=\"mbti_processed.csv\", tokenizer_path=\"models/tokenizer\")\n",
    "dm.setup()\n",
    "dl = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlbertTransformer(Module):\n",
    "    def __init__(self, model, d_head) -> None:\n",
    "        super().__init__()\n",
    "        self.albert = AutoModelForSequenceClassification.from_pretrained(model)\n",
    "\n",
    "    def forward(self, ids, mask) -> Tensor:\n",
    "        out = self.albert.albert(input_ids=ids, attention_mask=mask)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AlbertTransformer(, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-base-v2 were not used when initializing AlbertForSequenceClassification: ['predictions.decoder.weight', 'predictions.bias', 'predictions.decoder.bias', 'predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.LayerNorm.weight']\n",
      "- This IS expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "albert = AutoModelForSequenceClassification.from_pretrained(\"albert-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 481/481 [00:00<00:00, 239kB/s]\n",
      "Downloading: 100%|██████████| 478M/478M [00:13<00:00, 38.2MB/s] \n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "roberta = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in dl:\n",
    "    ids, mask, labels = batch[\"ids\"], batch[\"mask\"], batch[\"labels\"]\n",
    "    # print(batch)\n",
    "    print(ids.shape)\n",
    "    output = roberta(input_ids=ids,attention_mask=mask)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c5a6edcd19c60b54e1ba67f3a8dc1bfbd1b2a26eede9485e5dd135d032fa22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
